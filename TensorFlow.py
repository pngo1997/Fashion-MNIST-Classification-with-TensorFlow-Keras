# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/classification.ipynb

#### Student Name: Mai Ngo
#### Course Name and Number: CSC 578 Neural Network and Deep Learning - SEC 701
#### Homework 4 - Part 5: TensorFlow/Keras tutorial.
#### Date: 10/17/2023
"""

#TensorFlow and tf.keras
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
print(tf.__version__)

"""## Import the Fashion MNIST dataset"""

fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

"""The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:

<table>
  <tr>
    <th>Label</th>
    <th>Class</th>
  </tr>
  <tr>
    <td>0</td>
    <td>T-shirt/top</td>
  </tr>
  <tr>
    <td>1</td>
    <td>Trouser</td>
  </tr>
    <tr>
    <td>2</td>
    <td>Pullover</td>
  </tr>
    <tr>
    <td>3</td>
    <td>Dress</td>
  </tr>
    <tr>
    <td>4</td>
    <td>Coat</td>
  </tr>
    <tr>
    <td>5</td>
    <td>Sandal</td>
  </tr>
    <tr>
    <td>6</td>
    <td>Shirt</td>
  </tr>
    <tr>
    <td>7</td>
    <td>Sneaker</td>
  </tr>
    <tr>
    <td>8</td>
    <td>Bag</td>
  </tr>
    <tr>
    <td>9</td>
    <td>Ankle boot</td>
  </tr>
</table>
"""

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

train_images = train_images / 255.0
test_images = test_images / 255.0

"""## Build the model - Original

Follow the instruction requirement:

*   1st layer has to be Flattened with input_shape=(28, 28)
*   Last layer has to be a Dense layer with 10 nodes.
"""

originalModel = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)])

originalModel.compile(optimizer='Adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

originalModel.fit(train_images, train_labels, epochs=10)

test_loss, test_acc = originalModel.evaluate(test_images,  test_labels, verbose=2)
print('\nTest accuracy:', test_acc)

"""Original model has an accuracy of about:

*   91.11% on training data.
*   88.33% on testing data.

## 2nd Model - 2 hidden layers.

Follow the instruction requirement:
*   Expand Original Model.
*   2nd hidden layer of size 32.
"""

model2 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(10)])

model2.compile(optimizer='Adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

model2.fit(train_images, train_labels, epochs=10)

test_loss, test_acc = model2.evaluate(test_images,  test_labels, verbose=2)
print('\nTest accuracy:', test_acc)

"""Second model with 2 hidden layers has an accuracy of about:

*   90.99% on training data.
*   87.20% on testing data.

## 3rd Model - 2 hidden layers with learning rate of 0.01.
"""

model3 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(10)])

from tensorflow import keras
opt = keras.optimizers.Adam(learning_rate=0.01)
model3.compile(optimizer=opt,
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

model3.fit(train_images, train_labels, epochs=10)

test_loss, test_acc = model3.evaluate(test_images,  test_labels, verbose=2)
print('\nTest accuracy:', test_acc)

"""Third model with 2 hidden layers - learning rate of 0.01 has an accuracy of about:

*   87.37% on training data.
*   85.73% on testing data.

## 4th Model - 2 hidden layers with learning rate of 0.00025 and L2 regularization.
"""

from tensorflow.keras import regularizers
model4 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01), bias_regularizer=regularizers.L2(0.01), activity_regularizer=regularizers.L2(0.01)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.L2(0.01), bias_regularizer=regularizers.L2(0.01), activity_regularizer=regularizers.L2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10)])

opt = keras.optimizers.Adam(learning_rate=0.00025)
model4.compile(optimizer=opt,
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

model4.fit(train_images, train_labels, epochs=10)

test_loss, test_acc = model4.evaluate(test_images,  test_labels, verbose=2)
print('\nTest accuracy:', test_acc)

"""Third model with 2 hidden layers - learning rate of 0.01 has an accuracy of about:

*   83.23% on training data.
*   84.36% on testing data.

## Final Model - 2 hidden layers with learning rate of 0.00025, L2 regularization, and second hidden layer drop out = 0.2.
"""

model5 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.01), bias_regularizer=regularizers.L2(0.01), activity_regularizer=regularizers.L2(0.01)),
    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.L2(0.01), bias_regularizer=regularizers.L2(0.01), activity_regularizer=regularizers.L2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')])

opt = keras.optimizers.Adam(learning_rate=0.00025)
model5.compile(optimizer=opt,
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

model5.fit(train_images, train_labels, epochs=10)

test_loss, test_acc = model5.evaluate(test_images,  test_labels, verbose=2)
print('\nTest accuracy:', test_acc)

"""Third model with 2 hidden layers - learning rate of 0.01 has an accuracy of about:

*   85.04% on training data.
*   85.08% on testing data.
"""

finalModel = model5.fit(train_images, train_labels, epochs=50,
    validation_data=(test_images, test_labels))

print(finalModel.history.keys())

"""## Plot distribution of train and test set performance."""

import matplotlib.pyplot as plt
fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)

axes[0].plot(finalModel.history['accuracy'], label='train')
axes[0].plot(finalModel.history['val_accuracy'], label='val')
axes[0].set_title('Neural Network Accuracy')
axes[0].set_ylabel('accuracy')
axes[0].legend(loc='best')

axes[1].plot(finalModel.history['loss'], label='train')
axes[1].plot(finalModel.history['val_loss'], label='val')
axes[1].set_title('Neural Network CE Loss')
axes[1].set_ylabel('CE loss')
axes[1].set_xlabel('epoch')
axes[1].legend(loc='best')

plt.show()

"""## Make predictions."""

probabilityModel = tf.keras.Sequential([model5, tf.keras.layers.Softmax()])

predictions = probabilityModel.predict(test_images)

"""Look at the first prediction:"""

predictions[0]

np.argmax(predictions[0])

"""Same output as original model, the final model is most confident that this image is an ankle boot, or `class_names[9]`. Examining the test label shows that this classification is correct:"""

test_labels[0]

"""Define functions to graph the full set of 10 class predictions."""

def plot_image(i, predictions_array, true_label, img):
  true_label, img = true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
  true_label = true_label[i]
  plt.grid(False)
  plt.xticks(range(10))
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

"""## Verify predictions

Let's look at the 0th image, predictions, and prediction array. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percentage (out of 100) for the predicted label.
"""

i = 0
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()

"""Compare to the original model, the prediction confidence level is not that significantly standout. My thought is because I am applying L2 regularization which put penalty prevent extreme weight value. Hence, resonably distributed weight accross features (pixels)."""

i = 12
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()

"""Results are consistnt with orginal model."""

# Plot the first X test images, their predicted labels, and the true labels.
# Color correct predictions in blue and incorrect predictions in red.
num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_image(i, predictions[i], test_labels, test_images)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_value_array(i, predictions[i], test_labels)
plt.tight_layout()
plt.show()